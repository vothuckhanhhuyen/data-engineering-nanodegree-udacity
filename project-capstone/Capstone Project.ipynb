{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The purpose of this project is to create a data warehouse by combining immigration and demography data to create a comprehensive single-source-of-truth database.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import configparser\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType, IntegerType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import isnan, when, count, col, udf, dayofmonth, dayofweek, month, year, weekofyear, upper\n",
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder.\\\n",
    "            config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "            config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "            enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1.1. Scope \n",
    "- Data:\n",
    "    - I94 Immigration Data\n",
    "    - U.S. City Demographic Data\n",
    "- End solution description:\n",
    "    - This project builds an **ETL pipeline** for a **data lake**. We loaded data from S3, process the data into analytics tables using **Spark**, and load them back into S3.\n",
    "- Tools:\n",
    "    - Amazon S3\n",
    "    - Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1.2. Describe and Gather Data \n",
    "- **I94 Immigration Data:** \n",
    "    - [Source](https://www.trade.gov/national-travel-and-tourism-office): This data comes from the US National Tourism and Trade Office.\n",
    "    - Description: This dataset contains international visitor arrival statistics by world regions and select countries (including top 20), type of visa, mode of transportation, age groups, states visited (first intended address only), and the top ports of entry (for select countries).\n",
    "- **U.S. City Demographic Data:**\n",
    "    - [Source](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/): This data comes from OpenSoft.\n",
    "    - Description: This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### **I94 Immigration Data**\n",
    "- In this project, I choose to work with April data but you can work with any months in the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.listdir(\"../../data/18-83510-I94-Data-2016/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_immigration_spark = spark.read.format(\"com.github.saurfang.sas.spark\").load(\"../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\")\n",
    "print(\"Number of partitions: \", i94_immigration_spark.rdd.getNumPartitions())\n",
    "print(\"Number of records: \", i94_immigration_spark.rdd.count())\n",
    "i94_immigration_spark.printSchema()\n",
    "i94_immigration_spark.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### **U.S. City Demographic Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "uscities_demographic_spark = spark.read.option(\"header\", True).option(\n",
    "    \"delimiter\", \";\").option(\"inferSchema\", True).csv(\"raw_data/us-cities-demographics.csv\")\n",
    "print(\"Number of partitions: \", uscities_demographic_spark.rdd.getNumPartitions())\n",
    "print(\"Number of records: \", uscities_demographic_spark.rdd.count())\n",
    "uscities_demographic_spark.printSchema()\n",
    "uscities_demographic_spark.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.1. Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### **I94 Immigration Data**\n",
    "- Before exploring I94 Immigration data, I want to check out the SAS description file \"I94_SAS_Labels_Descriptions.SAS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_sas_des_file(sas_des_file, value, columns):\n",
    "    \"\"\"\n",
    "    Process SAS description file to return value as pandas dataframe\n",
    "    Arguments:\n",
    "        sas_des_file (str): SAS description file source.\n",
    "        value (str): sas value to extract.\n",
    "        columns (list): list of 2 containing column names.\n",
    "    Return:\n",
    "        pandas dataframe\n",
    "    \"\"\"\n",
    "    with open(sas_des_file) as f:\n",
    "        file = f.read()\n",
    "    \n",
    "    file = file[file.index(value):]\n",
    "    file = file[:file.index(\";\")]\n",
    "    \n",
    "    lines = file.split(\"\\n\")[1:]\n",
    "    codes = []\n",
    "    values = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if \"=\" in line:\n",
    "            code, val = line.split(\"=\")\n",
    "            code = code.strip()\n",
    "            val = val.strip()\n",
    "\n",
    "            if code[0] == \"'\":\n",
    "                code = code[1:-1]\n",
    "\n",
    "            if val[0] == \"'\":\n",
    "                val = val[1:-1]\n",
    "\n",
    "            codes.append(code)\n",
    "            values.append(val)\n",
    "        \n",
    "    return pd.DataFrame(list(zip(codes, values)), columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94cit_res = process_sas_des_file(\"raw_data/I94_SAS_Labels_Descriptions.SAS\", \"i94cntyl\", [\"code\", \"country\"])\n",
    "i94cit_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94cit_res.to_csv(\"clean_data/i94cit_res.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94port = process_sas_des_file(\"raw_data/I94_SAS_Labels_Descriptions.SAS\", \"i94prtl\", [\"code\", \"port\"])\n",
    "i94port.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94port.to_csv(\"clean_data/i94port.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94mode = process_sas_des_file(\"raw_data/I94_SAS_Labels_Descriptions.SAS\", \"i94model\", [\"code\", \"mode\"])\n",
    "i94mode.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94addr = process_sas_des_file(\"raw_data/I94_SAS_Labels_Descriptions.SAS\", \"i94addrl\", [\"code\", \"addr\"])\n",
    "i94addr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94addr.to_csv(\"clean_data/i94addr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94visa = process_sas_des_file(\"raw_data/I94_SAS_Labels_Descriptions.SAS\", \"I94VISA\", [\"code\", \"type\"])\n",
    "i94visa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_immigration_spark.select(*[\n",
    "    (\n",
    "        F.count(F.when((F.isnan(c) | F.col(c).isNull()), c)) if t not in (\"timestamp\", \"date\")\n",
    "        else F.count(F.when(F.col(c).isNull(), c))\n",
    "    ).alias(c)\n",
    "    for c, t in i94_immigration_spark.dtypes if c in i94_immigration_spark.columns\n",
    "]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### **U.S. City Demographic Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "uscities_demographic_spark.select(*[\n",
    "    (\n",
    "        F.count(F.when((F.isnan(c) | F.col(c).isNull()), c)) if t not in (\"timestamp\", \"date\")\n",
    "        else F.count(F.when(F.col(c).isNull(), c))\n",
    "    ).alias(c)\n",
    "    for c, t in uscities_demographic_spark.dtypes if c in uscities_demographic_spark.columns\n",
    "]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.2. Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### **I94 Immigration Data**\n",
    "- Drop columns \"occup\", \"entdepu\", \"insnum\" which have almost null values\n",
    "- Drop records that have all null columns\n",
    "- Select important columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def convert_sas_date(days):\n",
    "    \"\"\"\n",
    "    Convert SAS description file date stored as days since 01/01/1960 to datetime\n",
    "    params: days since 01/01/2016\n",
    "    return: datetime\n",
    "    \"\"\"\n",
    "    if days is None:\n",
    "        return None\n",
    "    return datetime.date(1960, 1, 1) + datetime.timedelta(days=days)\n",
    "\n",
    "def convert_sas_day_month(days):\n",
    "    \"\"\"\n",
    "    Convert SAS description file date stored as days since 01/01/1960 to day in month\n",
    "    params: days since 01/01/2016\n",
    "    return: datetime\n",
    "    \"\"\"\n",
    "    if days is None:\n",
    "        return None\n",
    "    return (datetime.date(1960, 1, 1) + datetime.timedelta(days=days)).day\n",
    "\n",
    "\n",
    "def convert_i94mode(mode):\n",
    "    \"\"\"\n",
    "    Convert i94mode to i94mode description in SAS file\n",
    "    params: i94mode\n",
    "    return: i94mode description\n",
    "    \"\"\"\n",
    "    if mode == 1:\n",
    "        return \"Air\"\n",
    "    elif mode == 2:\n",
    "        return \"Sea\"\n",
    "    elif mode == 3:\n",
    "        return \"Land\"\n",
    "    else:\n",
    "        return \"Not Reported\"\n",
    "\n",
    "\n",
    "def convert_i94visa(visa):\n",
    "    \"\"\"\n",
    "    Convert i94visa to i94visa description in SAS file\n",
    "    params: i94visa\n",
    "    return: i94visa description\n",
    "    \"\"\"\n",
    "    if visa == 1:\n",
    "        return \"Business\"\n",
    "    elif visa == 2:\n",
    "        return \"Pleasure\"\n",
    "    elif visa == 3:\n",
    "        return \"Student\"\n",
    "    else:\n",
    "        return \"Not Reported\"\n",
    "    \n",
    "convert_sas_date_udf = F.udf(convert_sas_date, DateType())\n",
    "convert_sas_day_month_udf = F.udf(convert_sas_day_month, IntegerType())\n",
    "convert_i94mode_udf = F.udf(convert_i94mode, StringType())\n",
    "convert_i94visa_udf = F.udf(convert_i94visa, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_immigration_spark = i94_immigration_spark.drop(\n",
    "    *[\"occup\", \"entdepu\", \"insnum\"])\n",
    "i94_immigration_spark = i94_immigration_spark.dropna(how='all')\n",
    "\n",
    "i94_immigration_spark = i94_immigration_spark.withColumn(\"arrival_date\", convert_sas_date_udf(i94_immigration_spark[\"arrdate\"])) \\\n",
    "    .withColumn(\"departure_date\", convert_sas_date_udf(i94_immigration_spark[\"depdate\"])) \\\n",
    "    .withColumn('arrival_day', convert_sas_day_month_udf(i94_immigration_spark['arrdate'])) \\\n",
    "    .withColumn(\"year\", i94_immigration_spark[\"i94yr\"].cast(IntegerType())) \\\n",
    "    .withColumn(\"month\", i94_immigration_spark[\"i94mon\"].cast(IntegerType())) \\\n",
    "    .withColumn(\"age\", i94_immigration_spark[\"i94bir\"].cast(IntegerType())) \\\n",
    "    .withColumn(\"birth_year\", i94_immigration_spark[\"biryear\"].cast(IntegerType())) \\\n",
    "    .withColumn(\"country_code\", i94_immigration_spark[\"i94cit\"].cast(IntegerType())) \\\n",
    "    .withColumn(\"city_code\", i94_immigration_spark[\"i94addr\"].cast(StringType())) \\\n",
    "    .withColumn(\"port_code\", i94_immigration_spark[\"i94port\"].cast(StringType())) \\\n",
    "    .withColumn(\"mode\", convert_i94mode_udf(i94_immigration_spark[\"i94mode\"])) \\\n",
    "    .withColumn(\"visa_category\", convert_i94visa_udf(i94_immigration_spark[\"i94visa\"]))\n",
    "\n",
    "i94_immigration_spark = i94_immigration_spark.select([\"arrival_date\", \"departure_date\", \"arrival_day\",\n",
    "                                                      \"year\", \"month\", \"age\", \"birth_year\", \"country_code\",\n",
    "                                                      \"city_code\", \"port_code\", \"mode\", \"visa_category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_immigration_spark.write.mode(\"append\").partitionBy(\"year\", \"month\", \"arrival_day\") \\\n",
    "    .parquet(\"clean_data/i94_immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Number of records (before cleaning): 3096313\n",
    "print(\"Number of partitions: \", i94_immigration_spark.rdd.getNumPartitions())\n",
    "print(\"Number of records (after cleaning): \", i94_immigration_spark.rdd.count())\n",
    "i94_immigration_spark.printSchema()\n",
    "i94_immigration_spark.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### **U.S. City Demographic Data**\n",
    "- Drop columns having null values\n",
    "- Drop duplicated samples based on \"City\", \"State\", \"State Code\", \"Race\"\n",
    "- Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "subset_cols = [\n",
    "    \"Male Population\",\n",
    "    \"Female Population\",\n",
    "    \"Number of Veterans\",\n",
    "    \"Foreign-born\",\n",
    "    \"Average Household Size\"\n",
    "]\n",
    "uscities_demographic_spark = uscities_demographic_spark.dropna(subset=subset_cols)\n",
    "uscities_demographic_spark = uscities_demographic_spark.drop_duplicates(\n",
    "    [\"City\", \"State\", \"State Code\", \"Race\"])\n",
    "\n",
    "uscities_demographic_spark = uscities_demographic_spark.withColumnRenamed(\"City\", \"city\") \\\n",
    "    .withColumnRenamed(\"State\", \"state\") \\\n",
    "    .withColumnRenamed(\"State Code\", \"state_code\") \\\n",
    "    .withColumnRenamed(\"Median Age\", \"median_age\") \\\n",
    "    .withColumnRenamed(\"Male Population\", \"male_population\") \\\n",
    "    .withColumnRenamed(\"Female Population\", \"female_population\") \\\n",
    "    .withColumnRenamed(\"Total Population\", \"total_population\") \\\n",
    "    .withColumnRenamed(\"Number of Veterans\", \"number_of_veterans\") \\\n",
    "    .withColumnRenamed(\"Foreign-born\", \"foreign_born\") \\\n",
    "    .withColumnRenamed(\"Average Household Size\", \"average_household_size\") \\\n",
    "    .withColumnRenamed(\"Race\", \"race\") \\\n",
    "    .withColumnRenamed(\"Count\", \"count\")\n",
    "\n",
    "uscities_demographic_spark = uscities_demographic_spark.withColumn(\"city_updated\", upper(col(\"city\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "uscities_demographic_spark.write.mode(\"overwrite\").parquet(\n",
    "    f\"clean_data/uscities_demographic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Number of records (before cleaning): 2891\n",
    "print(\"Number of partitions: \", uscities_demographic_spark.rdd.getNumPartitions())\n",
    "print(\"Number of records (after cleaning): \", uscities_demographic_spark.rdd.count())\n",
    "uscities_demographic_spark.printSchema()\n",
    "uscities_demographic_spark.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.1. Conceptual Data Model\n",
    "- We use snowflake schema.\n",
    "- The snowflake schema is similar to the star schema. However, in the snowflake schema, dimensions are normalized into multiple related tables, whereas the star schema's dimensions are denormalized with each dimension represented by a single table.\n",
    "![](data_model.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2. Mapping Out Data Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "0. Assume all data sets are stored in S3 buckets as below:\n",
    "    * `[Source_S3_Bucket]/immigration/18-83510-I94-Data-2016/*.sas7bdat`\n",
    "    * `[Source_S3_Bucket]/raw_data/I94_SAS_Labels_Descriptions.SAS`\n",
    "    * `[Source_S3_Bucket]/raw_data/us-cities-demographics.csv`\n",
    "1. Follow by Step 1 - Load the datasets\n",
    "2. Follow by Step 2\n",
    "    * Parsing label description file to get auxiliary tables\n",
    "    * Explore and clean the datasets\n",
    "3. Follow by Step 4\n",
    "    * Transform i94 immigration data and us cities demographics to 1 fact table and 5 dimension tables\n",
    "    * Store these tables back to target S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1. Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### **Create arrival date dimension table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "arrival_date_spark = i94_immigration_spark.select([\"arrival_date\"]).distinct().withColumnRenamed(\"arrival_date\", \"date\")\n",
    "arrival_date_spark = arrival_date_spark.withColumn(\"year\", year(\"date\"))\n",
    "arrival_date_spark = arrival_date_spark.withColumn(\"month\", month(\"date\"))\n",
    "arrival_date_spark = arrival_date_spark.withColumn(\"day\", dayofmonth(\"date\"))\n",
    "arrival_date_spark = arrival_date_spark.withColumn(\"week\", weekofyear(\"date\"))\n",
    "arrival_date_spark = arrival_date_spark.withColumn(\"day_of_week\", dayofweek(\"date\"))\n",
    "arrival_date_spark = arrival_date_spark.withColumn(\"date_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "arrival_date_spark.write.mode(\"overwrite\").parquet(\n",
    "    f\"clean_data/arrival_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of partitions: \", arrival_date_spark.rdd.getNumPartitions())\n",
    "print(\"Number of records: \", arrival_date_spark.rdd.count())\n",
    "arrival_date_spark.printSchema()\n",
    "arrival_date_spark.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### **Create country dimension table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94cit_res[\"code\"] = i94cit_res[\"code\"].apply(lambda x: int(x))\n",
    "dict_country = dict(sorted(i94cit_res.values.tolist()))\n",
    "\n",
    "@udf()\n",
    "def get_country_name(country_code):\n",
    "    if dict_country.get(country_code) is not None:\n",
    "        return dict_country.get(country_code)\n",
    "    return None\n",
    "\n",
    "country_spark = i94_immigration_spark.select([\"country_code\"]).distinct()\n",
    "country_spark = country_spark.withColumn(\"country_name\", get_country_name(country_spark[\"country_code\"]))\n",
    "country_spark = country_spark.withColumn(\"country_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "country_spark.write.mode(\"overwrite\").parquet(\n",
    "    f\"clean_data/country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of partitions: \", country_spark.rdd.getNumPartitions())\n",
    "print(\"Number of records: \", country_spark.rdd.count())\n",
    "country_spark.printSchema()\n",
    "country_spark.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### **Create port dimension table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dict_port = dict(sorted(i94port.values.tolist()))\n",
    "\n",
    "@udf()\n",
    "def get_port_name(port_code):\n",
    "    if dict_port.get(port_code) is not None:\n",
    "        return dict_port.get(port_code)\n",
    "    return None\n",
    "\n",
    "port_spark = i94_immigration_spark.select([\"port_code\"]).distinct()\n",
    "port_spark = port_spark.withColumn(\"port_name\", get_port_name(port_spark[\"port_code\"]))\n",
    "port_spark = port_spark.withColumn(\"port_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "port_spark.write.mode(\"overwrite\").parquet(\n",
    "    f\"clean_data/port\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of partitions: \", port_spark.rdd.getNumPartitions())\n",
    "print(\"Number of records: \", port_spark.rdd.count())\n",
    "port_spark.printSchema()\n",
    "port_spark.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### **Create city dimension table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dict_city = dict(sorted(i94addr.values.tolist()))\n",
    "\n",
    "@udf()\n",
    "def get_city_name(city_code):\n",
    "    if dict_city.get(city_code) is not None:\n",
    "        return dict_city.get(city_code)\n",
    "    return None\n",
    "\n",
    "city_spark = i94_immigration_spark.select([\"city_code\"]).distinct()\n",
    "city_spark = city_spark.withColumn(\"city_name\", get_city_name(city_spark[\"city_code\"])) \\\n",
    "    .withColumn(\"city_id\", monotonically_increasing_id())\n",
    "\n",
    "city_spark = city_spark.join(uscities_demographic_spark, (city_spark[\"city_name\"] == uscities_demographic_spark[\"city_updated\"]), \"left\") \\\n",
    "    .select(\"city_id\", \"city_code\", \"city_name\",\n",
    "           uscities_demographic_spark[\"state\"].alias(\"city_state\"),\n",
    "           uscities_demographic_spark[\"state_code\"].alias(\"city_state_code\")) \\\n",
    "    .distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "city_spark.write.mode(\"overwrite\").parquet(\n",
    "    f\"clean_data/city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of partitions: \", city_spark.rdd.getNumPartitions())\n",
    "print(\"Number of records: \", city_spark.rdd.count())\n",
    "city_spark.printSchema()\n",
    "city_spark.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### **Create us cities demographics dimension table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "uscities_demographic_spark = uscities_demographic_spark.join(city_spark, (uscities_demographic_spark[\"city_updated\"] == city_spark[\"city_name\"]), \"left\") \\\n",
    "    .distinct() \\\n",
    "    .select(\"city_id\", \"median_age\", \"male_population\",\n",
    "        \"female_population\", \"total_population\", \"number_of_veterans\",\n",
    "        \"foreign_born\", \"average_household_size\", \"race\", \"count\") \\\n",
    "    .withColumn(\"demographic_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "uscities_demographic_spark.write.mode(\"overwrite\").parquet(\n",
    "    f\"clean_data/uscities_demographic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of partitions: \", uscities_demographic_spark.rdd.getNumPartitions())\n",
    "print(\"Number of records: \", uscities_demographic_spark.rdd.count())\n",
    "uscities_demographic_spark.printSchema()\n",
    "uscities_demographic_spark.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### **Create i94 immigration fact table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_immigration_spark = i94_immigration_spark.withColumn(\"immigration_id\", monotonically_increasing_id())\n",
    "i94_immigration_spark = i94_immigration_spark.join(country_spark, (i94_immigration_spark[\"country_code\"] == country_spark[\"country_code\"]), \"left\").distinct() \\\n",
    "    .join(port_spark, (i94_immigration_spark[\"port_code\"] == port_spark[\"port_code\"]), \"left\").distinct() \\\n",
    "    .join(city_spark, (i94_immigration_spark[\"city_code\"] == city_spark[\"city_code\"]), \"left\").distinct() \\\n",
    "    .select(\"immigration_id\", \"arrival_date\", \"departure_date\", \"year\", \n",
    "            \"month\", \"age\", \"birth_year\", \"country_id\", \"city_id\", \n",
    "            \"port_id\", \"mode\", \"visa_category\") \\\n",
    "    .distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_immigration_spark.write.mode(\"overwrite\").parquet(\n",
    "    f\"clean_data/i94_immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of partitions: \", i94_immigration_spark.rdd.getNumPartitions())\n",
    "print(\"Number of records: \", i94_immigration_spark.rdd.count())\n",
    "i94_immigration_spark.printSchema()\n",
    "i94_immigration_spark.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2. Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def quality_check(df, df_name):\n",
    "    \"\"\"\n",
    "    Check count on fact and dimension table\n",
    "    params:\n",
    "        - df: dataframe to check count on\n",
    "        - df_name: name of dataframe\n",
    "    \"\"\"\n",
    "    count = df.count()\n",
    "    if count == 0:\n",
    "        print(f\"Data quality check failed for {df_name} table with zero record\")\n",
    "    else:\n",
    "        print(f\"Data quality check passed for {df_name} table with {count} records\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_dict = {\n",
    "    \"i94 immigration fact\": i94_immigration_spark,\n",
    "    \"us cities demographics dimension\": uscities_demographic_spark,\n",
    "    \"country dimension\": country_spark,\n",
    "    \"port dimension\": port_spark,\n",
    "    \"city dimension\": city_spark,\n",
    "    \"arrival date dimension\": arrival_date_spark\n",
    "}\n",
    "for df_name, df in df_dict.items():\n",
    "    quality_check(df, df_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3. Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from.\n",
    "\n",
    "##### **i94 immigration fact table**\n",
    "- immigration_id: long (nullable = false): Immigration ID\n",
    "- city_id: long (nullable = true): City ID\n",
    "- country_id: long (nullable = true): Country ID\n",
    "- port_id: long (nullable = true): Port ID\n",
    "- year: integer (nullable = true): 4 digits year\n",
    "- month: integer (nullable = true): Numeric month\n",
    "- age: integer (nullable = true): Age of respondent in years\n",
    "- birth_year: integer (nullable = true): 4 digits year of birth\n",
    "- mode: string (nullable = true): Mode of transportation\n",
    "- visa_category: string (nullable = true): Visa codes collapsed into three categories\n",
    "- arrival_date: date (nullable = true): Arrival date in the USA\n",
    "- departure_date: date (nullable = true): Departure date from the USA\n",
    "\n",
    "##### **us cities demographics dimension**\n",
    "- demographic_id: long (nullable = false): Demographic ID\n",
    "- city_id: long (nullable = true): City ID\n",
    "- median_age: double (nullable = true): Median age of the population\n",
    "- male_population: integer (nullable = true): Count of male population\n",
    "- female_population: integer (nullable = true): Count of female population\n",
    "- total_population: integer (nullable = true): Count of total population\n",
    "- number_of_veterans: integer (nullable = true): Count of total Veterans\n",
    "- foreign_born: integer (nullable = true): Count of foreign residents\n",
    "- average_household_size: double (nullable = true): Average city household size\n",
    "- race: string (nullable = true): Respondent race\n",
    "- count: integer (nullable = true): Count of city's individual per race\n",
    "\n",
    "##### **city dimension table**\n",
    "- city_id: long (nullable = false): City ID\n",
    "- city_code: string (nullable = true): City code\n",
    "- city_name: string (nullable = true): City name\n",
    "- city_state: string (nullable = true): US state where city is located\n",
    "- city_state_code: string (nullable = true): US state code where city is located\n",
    "\n",
    "##### **port dimension table**\n",
    "- port_id: long (nullable = false): Port of admission ID\n",
    "- port_code: string (nullable = true): Port code\n",
    "- port_name: string (nullable = true): port name\n",
    "\n",
    "##### **country dimension table**\n",
    "- country_id: long (nullable = false): country ID\n",
    "- country_code: integer (nullable = true): Country code\n",
    "- country_name: string (nullable = true): Country name\n",
    "\n",
    "##### **arrival date dimension table**\n",
    "- date_id: long (nullable = false): Date ID\n",
    "- date: date (nullable = true): Arrival date in the USA\n",
    "- year: integer (nullable = true): Arrival year\n",
    "- month: integer (nullable = true): Arrival month\n",
    "- day: integer (nullable = true): Arrival day of month\n",
    "- week: integer (nullable = true): Arrival week\n",
    "- day_of_week: integer (nullable = true): Arrival day of week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    " * Apache Spark was chosen because it is capable of processing large amounts of data in various file formats, offers a fast unified analytics engine for big data, and has user-friendly APIs for working with large datasets.\n",
    "* Propose how often the data should be updated and why.\n",
    "    1. The tables created from the immigration and temperature data sets should be updated on a monthly basis, as the raw data is collected monthly.\n",
    "    2. The tables created from the demography data set can be updated on an annual basis, as demography data collection takes time and frequent updates may be costly and could lead to incorrect conclusions.\n",
    "    3. All tables should be updated in an append-only manner.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "      * If the standalone server mode of Spark is unable to handle a data set that is 100 times larger, it may be necessary to use a distributed data cluster such as AWS EMR (Amazon Web Services Elastic MapReduce) to process the data on the cloud\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "     * Apache Airflow can be utilized to create an ETL (extract, transform, load) data pipeline that regularly updates the data and generates a report. It also has strong integration with Python and AWS, and can be used in conjunction with other tools to provide more advanced task automation capabilities.\n",
    " * The database needed to be accessed by 100+ people.\n",
    "     * AWS Redshift has the capacity to support up to 500 connections. If this database will be accessed by over 100 people, it may be a good idea to consider moving it to Redshift to ensure it can handle the workload. A cost-benefit analysis should be conducted before implementing this cloud solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
